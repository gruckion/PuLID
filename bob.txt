diff --git a/app.py b/app.py
index 6be73b6..df54b87 100644
--- a/app.py
+++ b/app.py
@@ -1,14 +1,30 @@
+import os
 import gradio as gr
 import numpy as np
 import torch
 
+# Enable MPS fallback for operations not supported by MPS
+os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
+
+# Use MPS if available
+if torch.backends.mps.is_available():
+    device = torch.device("mps")
+    # Use float32 to avoid half-precision errors
+    torch_dtype = torch.float32
+    print(f"Using MPS device with dtype: {torch_dtype}")
+else:
+    device = torch.device("cpu")
+    torch_dtype = torch.float32
+    print(f"MPS not available, using CPU with dtype: {torch_dtype}")
+
 from pulid import attention_processor as attention
 from pulid.pipeline import PuLIDPipeline
 from pulid.utils import resize_numpy_image_long, seed_everything
 
 torch.set_grad_enabled(False)
 
-pipeline = PuLIDPipeline()
+# Create pipeline with MPS device and float32 dtype
+pipeline = PuLIDPipeline(device=device, weight_dtype=torch_dtype)
 
 # other params
 DEFAULT_NEGATIVE_PROMPT = (
@@ -36,24 +52,38 @@ def run(*args):
     else:
         raise ValueError
 
+    id_embeddings = None
     if id_image is not None:
-        id_image = resize_numpy_image_long(id_image, 1024)
-        id_embeddings = pipeline.get_id_embedding(id_image)
-        for supp_id_image in supp_images:
-            if supp_id_image is not None:
-                supp_id_image = resize_numpy_image_long(supp_id_image, 1024)
-                supp_id_embeddings = pipeline.get_id_embedding(supp_id_image)
-                id_embeddings = torch.cat(
-                    (id_embeddings, supp_id_embeddings if id_mix else supp_id_embeddings[:, :5]), dim=1
-                )
-    else:
-        id_embeddings = None
+        try:
+            id_image = resize_numpy_image_long(id_image, 1024)
+            id_embeddings = pipeline.get_id_embedding(id_image)
+
+            for supp_id_image in supp_images:
+                if supp_id_image is not None:
+                    supp_id_image = resize_numpy_image_long(supp_id_image, 1024)
+                    supp_id_embeddings = pipeline.get_id_embedding(supp_id_image)
+                    id_embeddings = torch.cat(
+                        (id_embeddings, supp_id_embeddings if id_mix else supp_id_embeddings[:, :5]), dim=1
+                    )
+        except Exception as e:
+            print(f"Error processing image: {e}")
+            import traceback
+
+            traceback.print_exc()
+            return [], []
 
     seed_everything(seed)
     ims = []
-    for _ in range(n_samples):
-        img = pipeline.inference(prompt, (1, H, W), neg_prompt, id_embeddings, id_scale, scale, steps)[0]
-        ims.append(np.array(img))
+    try:
+        for _ in range(n_samples):
+            img = pipeline.inference(prompt, (1, H, W), neg_prompt, id_embeddings, id_scale, scale, steps)[0]
+            ims.append(np.array(img))
+    except Exception as e:
+        print(f"Error during inference: {e}")
+        import traceback
+
+        traceback.print_exc()
+        return [], []
 
     return ims, pipeline.debug_img_list
 
@@ -61,7 +91,7 @@ def run(*args):
 _HEADER_ = '''
 <h2><b>Official Gradio Demo</b></h2><h2><a href='https://github.com/ToTheBeginning/PuLID' target='_blank'><b>PuLID: Pure and Lightning ID Customization via Contrastive Alignment</b></a></h2>
 
-**PuLID** is a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model’s behavior.
+**PuLID** is a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model's behavior.
 
 Code: <a href='https://github.com/ToTheBeginning/PuLID' target='_blank'>GitHub</a>. Techenical report: <a href='https://arxiv.org/abs/2404.16022' target='_blank'>ArXiv</a>.
 
diff --git a/app_flux.py b/app_flux.py
index 254fda4..f11de3a 100644
--- a/app_flux.py
+++ b/app_flux.py
@@ -1,10 +1,23 @@
+import os
 import time
 
+# Disable MPS acceleration to avoid tensor type mismatches
+os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
+
 import gradio as gr
 import torch
 from einops import rearrange
 from PIL import Image
 
+# Check if MPS is available and set device accordingly
+if torch.backends.mps.is_available():
+    # Force CPU usage to avoid the MPS tensor type issues
+    default_device = "cpu"
+    print("MPS is available but using CPU to avoid tensor type issues")
+else:
+    default_device = "cpu"
+    print("Using CPU")
+
 from flux.sampling import denoise, get_noise, get_schedule, prepare, unpack
 from flux.util import (
     SamplingOptions,
@@ -32,6 +45,11 @@ def get_models(name: str, device: torch.device, offload: bool, fp8: bool):
 
 class FluxGenerator:
     def __init__(self, model_name: str, device: str, offload: bool, aggressive_offload: bool, args):
+        # Override device with default_device if it's "mps" to avoid MPS tensor issues
+        if device == "mps":
+            device = default_device
+            print(f"Overriding MPS device to {default_device}")
+
         self.device = torch.device(device)
         self.offload = offload
         self.aggressive_offload = aggressive_offload
@@ -42,31 +60,40 @@ class FluxGenerator:
             offload=self.offload,
             fp8=args.fp8,
         )
-        self.pulid_model = PuLIDPipeline(self.model, device="cpu" if offload else device, weight_dtype=torch.bfloat16,
-                                         onnx_provider=args.onnx_provider)
+        self.pulid_model = PuLIDPipeline(
+            self.model,
+            device="cpu" if offload else device,
+            weight_dtype=torch.bfloat16,
+            onnx_provider=args.onnx_provider,
+        )
         if offload:
-            self.pulid_model.face_helper.face_det.mean_tensor = self.pulid_model.face_helper.face_det.mean_tensor.to(torch.device("cuda"))
-            self.pulid_model.face_helper.face_det.device = torch.device("cuda")
-            self.pulid_model.face_helper.device = torch.device("cuda")
-            self.pulid_model.device = torch.device("cuda")
+            gpu_device = "mps" if torch.backends.mps.is_available() else "cpu"
+            if torch.cuda.is_available():
+                gpu_device = "cuda"
+            self.pulid_model.face_helper.face_det.mean_tensor = self.pulid_model.face_helper.face_det.mean_tensor.to(
+                gpu_device
+            )
+            self.pulid_model.face_helper.face_det.device = torch.device(gpu_device)
+            self.pulid_model.face_helper.device = torch.device(gpu_device)
+            self.pulid_model.device = torch.device(gpu_device)
         self.pulid_model.load_pretrain(args.pretrained_model, version=args.version)
 
     @torch.inference_mode()
     def generate_image(
-            self,
-            width,
-            height,
-            num_steps,
-            start_step,
-            guidance,
-            seed,
-            prompt,
-            id_image=None,
-            id_weight=1.0,
-            neg_prompt="",
-            true_cfg=1.0,
-            timestep_to_start_cfg=1,
-            max_sequence_length=128,
+        self,
+        width,
+        height,
+        num_steps,
+        start_step,
+        guidance,
+        seed,
+        prompt,
+        id_image=None,
+        id_weight=1.0,
+        neg_prompt="",
+        true_cfg=1.0,
+        timestep_to_start_cfg=1,
+        max_sequence_length=128,
     ):
         self.t5.max_length = max_sequence_length
 
@@ -106,7 +133,16 @@ class FluxGenerator:
         )
 
         if self.offload:
-            self.t5, self.clip = self.t5.to(self.device), self.clip.to(self.device)
+            self.t5, self.clip = self.t5.cpu(), self.clip.cpu()
+            if torch.cuda.is_available():
+                torch.cuda.empty_cache()
+            elif torch.backends.mps.is_available():
+                torch.mps.empty_cache()
+            gpu_device = "mps" if torch.backends.mps.is_available() else "cpu"
+            if torch.cuda.is_available():
+                gpu_device = "cuda"
+            self.pulid_model.components_to_device(torch.device(gpu_device))
+
         inp = prepare(t5=self.t5, clip=self.clip, img=x, prompt=opts.prompt)
         inp_neg = prepare(t5=self.t5, clip=self.clip, img=x, prompt=neg_prompt) if use_true_cfg else None
 
@@ -134,8 +170,15 @@ class FluxGenerator:
 
         # denoise initial noise
         x = denoise(
-            self.model, **inp, timesteps=timesteps, guidance=opts.guidance, id=id_embeddings, id_weight=id_weight,
-            start_step=start_step, uncond_id=uncond_id_embeddings, true_cfg=true_cfg,
+            self.model,
+            **inp,
+            timesteps=timesteps,
+            guidance=opts.guidance,
+            id=id_embeddings,
+            id_weight=id_weight,
+            start_step=start_step,
+            uncond_id=uncond_id_embeddings,
+            true_cfg=true_cfg,
             timestep_to_start_cfg=timestep_to_start_cfg,
             neg_txt=inp_neg["txt"] if use_true_cfg else None,
             neg_txt_ids=inp_neg["txt_ids"] if use_true_cfg else None,
@@ -169,6 +212,7 @@ class FluxGenerator:
         img = Image.fromarray((127.5 * (x + 1.0)).cpu().byte().numpy())
         return img, str(opts.seed), self.pulid_model.debug_img_list
 
+
 _HEADER_ = '''
 <div style="text-align: center; max-width: 650px; margin: 0 auto;">
     <h1 style="font-size: 2.5rem; font-weight: 700; margin-bottom: 1rem; display: contents;">PuLID for FLUX</h1>
@@ -192,8 +236,13 @@ If you have any questions or feedbacks, feel free to open a discussion or contac
 """  # noqa E501
 
 
-def create_demo(args, model_name: str, device: str = "cuda" if torch.cuda.is_available() else "cpu",
-                offload: bool = False, aggressive_offload: bool = False):
+def create_demo(
+    args,
+    model_name: str,
+    device: str = "cuda" if torch.cuda.is_available() else "cpu",
+    offload: bool = False,
+    aggressive_offload: bool = False,
+):
     generator = FluxGenerator(model_name, device, offload, aggressive_offload, args)
 
     with gr.Blocks() as demo:
@@ -211,15 +260,22 @@ def create_demo(args, model_name: str, device: str = "cuda" if torch.cuda.is_ava
                 start_step = gr.Slider(0, 10, 0, step=1, label="timestep to start inserting ID")
                 guidance = gr.Slider(1.0, 10.0, 4, step=0.1, label="Guidance")
                 seed = gr.Textbox(-1, label="Seed (-1 for random)")
-                max_sequence_length = gr.Slider(128, 512, 128, step=128,
-                                                label="max_sequence_length for prompt (T5), small will be faster")
-
-                with gr.Accordion("Advanced Options (True CFG, true_cfg_scale=1 means use fake CFG, >1 means use true CFG, if using true CFG, we recommend set the guidance scale to 1)", open=False):    # noqa E501
+                max_sequence_length = gr.Slider(
+                    128, 512, 128, step=128, label="max_sequence_length for prompt (T5), small will be faster"
+                )
+
+                with gr.Accordion(
+                    "Advanced Options (True CFG, true_cfg_scale=1 means use fake CFG, >1 means use true CFG, if using true CFG, we recommend set the guidance scale to 1)",
+                    open=False,
+                ):  # noqa E501
                     neg_prompt = gr.Textbox(
                         label="Negative Prompt",
-                        value="bad quality, worst quality, text, signature, watermark, extra limbs")
+                        value="bad quality, worst quality, text, signature, watermark, extra limbs",
+                    )
                     true_cfg = gr.Slider(1.0, 10.0, 1, step=0.1, label="true CFG scale")
-                    timestep_to_start_cfg = gr.Slider(0, 20, 1, step=1, label="timestep to start cfg", visible=args.dev)
+                    timestep_to_start_cfg = gr.Slider(
+                        0, 20, 1, step=1, label="timestep to start cfg", visible=args.dev
+                    )
 
                 generate_btn = gr.Button("Generate")
 
@@ -230,71 +286,78 @@ def create_demo(args, model_name: str, device: str = "cuda" if torch.cuda.is_ava
                 gr.Markdown(_CITE_)
 
         with gr.Row(), gr.Column():
-                gr.Markdown("## Examples")
-                example_inps = [
-                    [
-                        'a woman holding sign with glowing green text \"PuLID for FLUX\"',
-                        'example_inputs/liuyifei.png',
-                        4, 4, 2680261499100305976, 1
-                    ],
-                    [
-                        'portrait, side view',
-                        'example_inputs/liuyifei.png',
-                        4, 4, 1205240166692517553, 1
-                    ],
-                    [
-                        'white-haired woman with vr technology atmosphere, revolutionary exceptional magnum with remarkable details',  # noqa E501
-                        'example_inputs/liuyifei.png',
-                        4, 4, 6349424134217931066, 1
-                    ],
-                    [
-                        'a young child is eating Icecream',
-                        'example_inputs/liuyifei.png',
-                        4, 4, 10606046113565776207, 1
-                    ],
-                    [
-                        'a man is holding a sign with text \"PuLID for FLUX\", winter, snowing, top of the mountain',
-                        'example_inputs/pengwei.jpg',
-                        4, 4, 2410129802683836089, 1
-                    ],
-                    [
-                        'portrait, candle light',
-                        'example_inputs/pengwei.jpg',
-                        4, 4, 17522759474323955700, 1
-                    ],
-                    [
-                        'profile shot dark photo of a 25-year-old male with smoke escaping from his mouth, the backlit smoke gives the image an ephemeral quality, natural face, natural eyebrows, natural skin texture, award winning photo, highly detailed face, atmospheric lighting, film grain, monochrome',  # noqa E501
-                        'example_inputs/pengwei.jpg',
-                        4, 4, 17733156847328193625, 1
-                    ],
-                    [
-                        'American Comics, 1boy',
-                        'example_inputs/pengwei.jpg',
-                        1, 4, 13223174453874179686, 1
-                    ],
-                    [
-                        'portrait, pixar',
-                        'example_inputs/pengwei.jpg',
-                        1, 4, 9445036702517583939, 1
-                    ],
-                ]
-                gr.Examples(examples=example_inps, inputs=[prompt, id_image, start_step, guidance, seed, true_cfg],
-                            label='fake CFG')
-
-                example_inps = [
-                    [
-                        'portrait, made of ice sculpture',
-                        'example_inputs/lecun.jpg',
-                        1, 1, 3811899118709451814, 5
-                    ],
-                ]
-                gr.Examples(examples=example_inps, inputs=[prompt, id_image, start_step, guidance, seed, true_cfg],
-                            label='true CFG')
+            gr.Markdown("## Examples")
+            example_inps = [
+                [
+                    'a woman holding sign with glowing green text \"PuLID for FLUX\"',
+                    'example_inputs/liuyifei.png',
+                    4,
+                    4,
+                    2680261499100305976,
+                    1,
+                ],
+                ['portrait, side view', 'example_inputs/liuyifei.png', 4, 4, 1205240166692517553, 1],
+                [
+                    'white-haired woman with vr technology atmosphere, revolutionary exceptional magnum with remarkable details',  # noqa E501
+                    'example_inputs/liuyifei.png',
+                    4,
+                    4,
+                    6349424134217931066,
+                    1,
+                ],
+                ['a young child is eating Icecream', 'example_inputs/liuyifei.png', 4, 4, 10606046113565776207, 1],
+                [
+                    'a man is holding a sign with text \"PuLID for FLUX\", winter, snowing, top of the mountain',
+                    'example_inputs/pengwei.jpg',
+                    4,
+                    4,
+                    2410129802683836089,
+                    1,
+                ],
+                ['portrait, candle light', 'example_inputs/pengwei.jpg', 4, 4, 17522759474323955700, 1],
+                [
+                    'profile shot dark photo of a 25-year-old male with smoke escaping from his mouth, the backlit smoke gives the image an ephemeral quality, natural face, natural eyebrows, natural skin texture, award winning photo, highly detailed face, atmospheric lighting, film grain, monochrome',  # noqa E501
+                    'example_inputs/pengwei.jpg',
+                    4,
+                    4,
+                    17733156847328193625,
+                    1,
+                ],
+                ['American Comics, 1boy', 'example_inputs/pengwei.jpg', 1, 4, 13223174453874179686, 1],
+                ['portrait, pixar', 'example_inputs/pengwei.jpg', 1, 4, 9445036702517583939, 1],
+            ]
+            gr.Examples(
+                examples=example_inps,
+                inputs=[prompt, id_image, start_step, guidance, seed, true_cfg],
+                label='fake CFG',
+            )
+
+            example_inps = [
+                ['portrait, made of ice sculpture', 'example_inputs/lecun.jpg', 1, 1, 3811899118709451814, 5],
+            ]
+            gr.Examples(
+                examples=example_inps,
+                inputs=[prompt, id_image, start_step, guidance, seed, true_cfg],
+                label='true CFG',
+            )
 
         generate_btn.click(
             fn=generator.generate_image,
-            inputs=[width, height, num_steps, start_step, guidance, seed, prompt, id_image, id_weight, neg_prompt,
-                    true_cfg, timestep_to_start_cfg, max_sequence_length],
+            inputs=[
+                width,
+                height,
+                num_steps,
+                start_step,
+                guidance,
+                seed,
+                prompt,
+                id_image,
+                id_weight,
+                neg_prompt,
+                true_cfg,
+                timestep_to_start_cfg,
+                max_sequence_length,
+            ],
             outputs=[output_image, seed_output, intermediate_output],
         )
 
@@ -305,16 +368,28 @@ if __name__ == "__main__":
     import argparse
 
     parser = argparse.ArgumentParser(description="PuLID for FLUX.1-dev")
-    parser.add_argument('--version', type=str, default='v0.9.1', help='version of the model', choices=['v0.9.0', 'v0.9.1'])
-    parser.add_argument("--name", type=str, default="flux-dev", choices=list('flux-dev'),
-                        help="currently only support flux-dev")
+    parser.add_argument(
+        '--version', type=str, default='v0.9.1', help='version of the model', choices=['v0.9.0', 'v0.9.1']
+    )
+    parser.add_argument(
+        "--name", type=str, default="flux-dev", choices=list('flux-dev'), help="currently only support flux-dev"
+    )
     parser.add_argument("--device", type=str, default="cuda", help="Device to use")
     parser.add_argument("--offload", action="store_true", help="Offload model to CPU when not in use")
-    parser.add_argument("--aggressive_offload", action="store_true", help="Offload model more aggressively to CPU when not in use, for 24G GPUs")
+    parser.add_argument(
+        "--aggressive_offload",
+        action="store_true",
+        help="Offload model more aggressively to CPU when not in use, for 24G GPUs",
+    )
     parser.add_argument("--fp8", action="store_true", help="use flux-dev-fp8 model")
-    parser.add_argument("--onnx_provider", type=str, default="gpu", choices=["gpu", "cpu"],
-                        help="set onnx_provider to cpu (default gpu) can help reduce RAM usage, and when combined with"
-                             "fp8 option, the peak RAM is under 15GB")
+    parser.add_argument(
+        "--onnx_provider",
+        type=str,
+        default="gpu",
+        choices=["gpu", "cpu"],
+        help="set onnx_provider to cpu (default gpu) can help reduce RAM usage, and when combined with"
+        "fp8 option, the peak RAM is under 15GB",
+    )
     parser.add_argument("--port", type=int, default=8080, help="Port to use")
     parser.add_argument("--dev", action='store_true', help="Development mode")
     parser.add_argument("--pretrained_model", type=str, help='for development')
diff --git a/app_v1_1.py b/app_v1_1.py
index 05815b7..7997759 100644
--- a/app_v1_1.py
+++ b/app_v1_1.py
@@ -1,9 +1,21 @@
 import argparse
+import os
+
+# Disable MPS acceleration to avoid tensor type mismatches
+os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
 
 import gradio as gr
 import numpy as np
 import torch
 
+# Force CPU usage to avoid MPS type mismatches
+if torch.backends.mps.is_available():
+    print("MPS is available but using CPU to avoid tensor type issues")
+    device = torch.device("cpu")
+else:
+    device = torch.device("cpu")
+    print("Using CPU")
+
 from pulid import attention_processor as attention
 from pulid.pipeline_v1_1 import PuLIDPipeline
 from pulid.utils import resize_numpy_image_long
@@ -35,7 +47,8 @@ else:
     default_cfg = 7.0
     default_steps = 25
 
-pipeline = PuLIDPipeline(sdxl_repo=args.base, sampler=args.sampler)
+# Initialize the pipeline with the CPU device
+pipeline = PuLIDPipeline(sdxl_repo=args.base, sampler=args.sampler, device=device)
 
 # other params
 DEFAULT_NEGATIVE_PROMPT = (
@@ -146,7 +159,7 @@ def run(*args):
 _HEADER_ = '''
 <h2><b>Official Gradio Demo</b></h2><h2><a href='https://github.com/ToTheBeginning/PuLID' target='_blank'><b>PuLID: Pure and Lightning ID Customization via Contrastive Alignment</b></a></h2>
 
-**PuLID** is a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model’s behavior.
+**PuLID** is a tuning-free ID customization approach. PuLID maintains high ID fidelity while effectively reducing interference with the original model's behavior.
 
 Code: <a href='https://github.com/ToTheBeginning/PuLID' target='_blank'>GitHub</a>. Paper: <a href='https://arxiv.org/abs/2404.16022' target='_blank'>ArXiv</a>.
 
diff --git a/flux/model.py b/flux/model.py
index 846b42d..b60f5f0 100644
--- a/flux/model.py
+++ b/flux/model.py
@@ -12,7 +12,11 @@ from flux.modules.layers import (
     timestep_embedding,
 )
 
-DEVICE = torch.device("cuda")
+
+DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
+if torch.cuda.is_available():
+    DEVICE = torch.device("cuda")
+
 
 @dataclass
 class FluxParams:
@@ -42,9 +46,7 @@ class Flux(nn.Module):
         self.in_channels = params.in_channels
         self.out_channels = self.in_channels
         if params.hidden_size % params.num_heads != 0:
-            raise ValueError(
-                f"Hidden size {params.hidden_size} must be divisible by num_heads {params.num_heads}"
-            )
+            raise ValueError(f"Hidden size {params.hidden_size} must be divisible by num_heads {params.num_heads}")
         pe_dim = params.hidden_size // params.num_heads
         if sum(params.axes_dim) != pe_dim:
             raise ValueError(f"Got {params.axes_dim} but expected positional dim {pe_dim}")
@@ -131,14 +133,14 @@ class Flux(nn.Module):
             for i in range(len(self.single_blocks) // 2):
                 self.single_blocks[i] = self.single_blocks[i].to(DEVICE)
         for i, block in enumerate(self.single_blocks):
-            if aggressive_offload and i == len(self.single_blocks)//2:
+            if aggressive_offload and i == len(self.single_blocks) // 2:
                 # put first half of the single blcoks to cpu and last half to gpu
                 for j in range(len(self.single_blocks) // 2):
                     self.single_blocks[j].cpu()
                 for j in range(len(self.single_blocks) // 2, len(self.single_blocks)):
                     self.single_blocks[j] = self.single_blocks[j].to(DEVICE)
             x = block(img, vec=vec, pe=pe)
-            real_img, txt = x[:, txt.shape[1]:, ...], x[:, :txt.shape[1], ...]
+            real_img, txt = x[:, txt.shape[1] :, ...], x[:, : txt.shape[1], ...]
 
             if i % self.pulid_single_interval == 0 and id is not None:
                 real_img = real_img + id_weight * self.pulid_ca[ca_idx](id, real_img)
diff --git a/flux/util.py b/flux/util.py
index 7ad5cae..380e2a6 100644
--- a/flux/util.py
+++ b/flux/util.py
@@ -111,7 +111,11 @@ def print_load_warning(missing: list[str], unexpected: list[str]) -> None:
         print(f"Got {len(unexpected)} unexpected keys:\n\t" + "\n\t".join(unexpected))
 
 
-def load_flow_model(name: str, device: str = "cuda", hf_download: bool = True):
+def load_flow_model(
+    name: str, device: str = "mps" if torch.backends.mps.is_available() else "cpu", hf_download: bool = True
+):
+    if torch.cuda.is_available():
+        device = "cuda"
     # Loading Flux
     print("Init model")
     ckpt_path = configs[name].ckpt_path
@@ -134,15 +138,17 @@ def load_flow_model(name: str, device: str = "cuda", hf_download: bool = True):
         print_load_warning(missing, unexpected)
     return model
 
+
 # from XLabs-AI https://github.com/XLabs-AI/x-flux/blob/1f8ef54972105ad9062be69fe6b7f841bce02a08/src/flux/util.py#L330
-def load_flow_model_quintized(name: str, device: str = "cuda", hf_download: bool = True):
+def load_flow_model_quintized(
+    name: str, device: str = "mps" if torch.backends.mps.is_available() else "cpu", hf_download: bool = True
+):
+    if torch.cuda.is_available():
+        device = "cuda"
     # Loading Flux
     print("Init model")
     ckpt_path = 'models/flux-dev-fp8.safetensors'
-    if (
-        not os.path.exists(ckpt_path)
-        and hf_download
-    ):
+    if not os.path.exists(ckpt_path) and hf_download:
         ckpt_path = hf_hub_download("XLabs-AI/flux-dev-fp8", "flux-dev-fp8.safetensors")
     json_path = hf_hub_download("XLabs-AI/flux-dev-fp8", 'flux_dev_quantization_map.json')
 
@@ -155,21 +161,30 @@ def load_flow_model_quintized(name: str, device: str = "cuda", hf_download: bool
         quantization_map = json.load(f)
     print("Start a quantization process...")
     from optimum.quanto import requantize
+
     requantize(model, sd, quantization_map, device=device)
     print("Model is quantized!")
     return model
 
 
-def load_t5(device: str = "cuda", max_length: int = 512) -> HFEmbedder:
+def load_t5(device: str = "mps" if torch.backends.mps.is_available() else "cpu", max_length: int = 512) -> HFEmbedder:
+    if torch.cuda.is_available():
+        device = "cuda"
     # max length 64, 128, 256 and 512 should work (if your sequence is short enough)
     return HFEmbedder("xlabs-ai/xflux_text_encoders", max_length=max_length, torch_dtype=torch.bfloat16).to(device)
 
 
-def load_clip(device: str = "cuda") -> HFEmbedder:
+def load_clip(device: str = "mps" if torch.backends.mps.is_available() else "cpu") -> HFEmbedder:
+    if torch.cuda.is_available():
+        device = "cuda"
     return HFEmbedder("openai/clip-vit-large-patch14", max_length=77, torch_dtype=torch.bfloat16).to(device)
 
 
-def load_ae(name: str, device: str = "cuda", hf_download: bool = True) -> AutoEncoder:
+def load_ae(
+    name: str, device: str = "mps" if torch.backends.mps.is_available() else "cpu", hf_download: bool = True
+) -> AutoEncoder:
+    if torch.cuda.is_available():
+        device = "cuda"
     ckpt_path = configs[name].ae_path
     if (
         not os.path.exists(ckpt_path)
diff --git a/pulid/pipeline.py b/pulid/pipeline.py
index 3e83a3e..ab28a4a 100644
--- a/pulid/pipeline.py
+++ b/pulid/pipeline.py
@@ -29,24 +29,70 @@ else:
     from pulid.attention_processor import AttnProcessor, IDAttnProcessor
 
 
-class PuLIDPipeline:
-    def __init__(self, *args, **kwargs):
+class PuLIDPipeline(nn.Module):
+    def __init__(self, device=None, weight_dtype=None):
         super().__init__()
-        self.device = 'cuda'
+        if device is None:
+            self.device = "mps" if torch.backends.mps.is_available() else "cpu"
+            if torch.cuda.is_available():
+                self.device = "cuda"
+        else:
+            self.device = device
+
+        # Determine appropriate dtype
+        if weight_dtype is not None:
+            self.weight_dtype = weight_dtype
+        else:
+            # Always use float32 for MPS or CPU to avoid half precision issues
+            is_cpu_or_mps = (
+                self.device == "mps"
+                or self.device == "cpu"
+                or (hasattr(self.device, 'type') and (self.device.type == "mps" or self.device.type == "cpu"))
+            )
+            if is_cpu_or_mps:
+                self.weight_dtype = torch.float32
+                print(f"Using float32 for {self.device} device")
+            else:
+                self.weight_dtype = torch.bfloat16
+
+        # Print configuration
+        print(f"Pipeline initialized with device: {self.device}, dtype: {self.weight_dtype}")
+
         sdxl_base_repo = 'stabilityai/stable-diffusion-xl-base-1.0'
         sdxl_lightning_repo = 'ByteDance/SDXL-Lightning'
         self.sdxl_base_repo = sdxl_base_repo
 
         # load base model
-        unet = UNet2DConditionModel.from_config(sdxl_base_repo, subfolder='unet').to(self.device, torch.float16)
-        unet.load_state_dict(
-            load_file(
-                hf_hub_download(sdxl_lightning_repo, 'sdxl_lightning_4step_unet.safetensors'), device=self.device
-            )
+        unet = UNet2DConditionModel.from_config(sdxl_base_repo, subfolder='unet').to(self.device, self.weight_dtype)
+
+        # Handle safetensors loading safely
+        safetensors_path = hf_hub_download(sdxl_lightning_repo, 'sdxl_lightning_4step_unet.safetensors')
+
+        # For MPS/CPU, load the weights to CPU first, then transfer to device
+        is_cpu_or_mps = (
+            self.device == "mps"
+            or self.device == "cpu"
+            or (hasattr(self.device, 'type') and (self.device.type == "mps" or self.device.type == "cpu"))
         )
+        if is_cpu_or_mps:
+            # Load without device specification for MPS/CPU
+            print(f"Loading safetensors without device specification for {self.device}")
+            state_dict = load_file(safetensors_path)
+            unet.load_state_dict(state_dict)
+        else:
+            # For CUDA devices with float16/bfloat16, use device specification
+            print(f"Loading safetensors with device specification: {self.device}")
+            unet.load_state_dict(load_file(safetensors_path, device=self.device))
+
         self.hack_unet_attn_layers(unet)
+
+        # Set variant to None for float32 to avoid fp16 issues
+        variant = None if self.weight_dtype == torch.float32 else "fp16"
+
+        # For MPS, need to load models in a specific sequence
+        print(f"Loading SDXL pipeline with dtype: {self.weight_dtype}, variant: {variant}")
         self.pipe = StableDiffusionXLPipeline.from_pretrained(
-            sdxl_base_repo, unet=unet, torch_dtype=torch.float16, variant="fp16"
+            sdxl_base_repo, unet=unet, torch_dtype=self.weight_dtype, variant=variant
         ).to(self.device)
         self.pipe.watermark = None
 
@@ -92,7 +138,10 @@ class PuLIDPipeline:
         self.handler_ante.prepare(ctx_id=0)
 
         gc.collect()
-        torch.cuda.empty_cache()
+        if torch.cuda.is_available():
+            torch.cuda.empty_cache()
+        elif hasattr(torch, 'mps') and torch.backends.mps.is_available():
+            torch.mps.empty_cache()
 
         self.load_pretrain()
 
diff --git a/pulid/pipeline_v1_1.py b/pulid/pipeline_v1_1.py
index 4ca68cf..6560691 100644
--- a/pulid/pipeline_v1_1.py
+++ b/pulid/pipeline_v1_1.py
@@ -29,14 +29,37 @@ else:
 
 
 class PuLIDPipeline:
-    def __init__(self, sdxl_repo='Lykon/dreamshaper-xl-lightning', sampler='dpmpp_sde', *args, **kwargs):
+    def __init__(self, sdxl_repo=None, sampler='dpmpp_2m', device=None):
         super().__init__()
-        self.device = 'cuda'
+        if device is None:
+            self.device = "mps" if torch.backends.mps.is_available() else "cpu"
+            if torch.cuda.is_available():
+                self.device = "cuda"
+        else:
+            self.device = device
+
+        # Set appropriate dtype based on device
+        if (
+            isinstance(self.device, str)
+            and self.device == "cpu"
+            or (hasattr(self.device, 'type') and self.device.type == "cpu")
+        ):
+            self.weight_dtype = torch.float32
+            print("Using float32 on CPU device")
+        else:
+            self.weight_dtype = torch.float16
+
+        # Load SDXL model with appropriate dtype for the device
+        if sdxl_repo is None:
+            sdxl_repo = 'stabilityai/stable-diffusion-xl-base-1.0'
+
+        # Set variant to None for CPU to avoid fp16 issues
+        variant = None if self.weight_dtype == torch.float32 else "fp16"
+
+        self.pipe = StableDiffusionXLPipeline.from_pretrained(
+            sdxl_repo, torch_dtype=self.weight_dtype, variant=variant
+        ).to(self.device)
 
-        # load base model
-        self.pipe = StableDiffusionXLPipeline.from_pretrained(sdxl_repo, torch_dtype=torch.float16, variant="fp16").to(
-            self.device
-        )
         self.pipe.watermark = None
         self.hack_unet_attn_layers(self.pipe.unet)
 
diff --git a/pyproject.toml b/pyproject.toml
index 810d0a2..0de3883 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -10,8 +10,8 @@ select = ["E", "F", "W", "C90", "I", "UP", "B", "C4", "RET", "RUF", "SIM"]
 
 
 ignore = [
-    "UP006",    # UP006: Use list instead of typing.List for type annotations
-    "UP007",    # UP007: Use X | Y for type annotations
+    "UP006",  # UP006: Use list instead of typing.List for type annotations
+    "UP007",  # UP007: Use X | Y for type annotations
     "UP009",
     "UP035",
     "UP038",
@@ -26,5 +26,14 @@ skip_glob = 'eva_clip/*.py'
 
 [tool.black]
 line-length = 119
-skip-string-normalization = 1
+skip-string-normalization = true
 exclude = 'eva_clip'
+
+[project]
+name = "pulid"
+version = "0.1.0"
+description = "PuLID"
+requires-python = ">=3.8"
+
+[tool.setuptools]
+packages = ["pulid"]
diff --git a/requirements.txt b/requirements.txt
index 081da6e..7edd21a 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -11,8 +11,13 @@ ftfy
 facexlib
 insightface
 onnxruntime
-onnxruntime-gpu
+onnxruntime-silicon
 accelerate
 SentencePiece
 safetensors
-torchsde
\ No newline at end of file
+torchsde
+huggingface-hub
+fastapi==0.109.0
+pydantic==2.6.3
+starlette<0.37.0
+uvicorn[standard]
diff --git a/requirements_fp8.txt b/requirements_fp8.txt
index 7159e9c..39a8a12 100644
--- a/requirements_fp8.txt
+++ b/requirements_fp8.txt
@@ -15,4 +15,5 @@ onnxruntime
 onnxruntime-gpu
 accelerate
 SentencePiece
-safetensors
\ No newline at end of file
+safetensors
+huggingface-hub
